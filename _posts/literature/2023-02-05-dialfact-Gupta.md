---
title: "DialFact: A Benchmark for Fact-Checking in Dialogue"
authors: Prakhar Gupta; Chien-Sheng Wu; Wenhao Liu; Caiming Xiong
conference: ACL 2022
arXivPubdate: "2021-10-15"
github: "https://github.com/salesforce/DialFact"
tags: [Dialogue, Fact-checking, Dataset, Verification, Evidence Retrieval]
description: "대화형 사실 검증을 위한 최초의 벤치마크 데이터셋과 평가 방법들을 제시한 연구"
categories:
  - literature
published: true
---

# DialFact: A Benchmark for Fact-Checking in Dialogue

## 문제 정의
온라인 대화에서 잘못된 정보(misinformation)의 확산은 사회적으로 큰 위험을 초래하며, COVID-19 팬데믹 등의 사례에서 그 해악이 드러났다. 최근 **대화형 인공지능**의 발전으로 챗봇이 사실과 다른 **hallucination** 정보를 생성하거나, 이를 악용해 거짓 주장을 대량 유포할 수 있어 대화 맥락에서의 **fact-checking** 필요성이 높아졌다. 그러나 기존 **사실 검증** 연구는 주로 위키백과 문장, 뉴스, 소셜미디어 등 공식적이고 정형화된 텍스트에 집중되어 왔고 【(Thorne et al., 2018)】【(Nakov et al., 2021)】, **대화** 영역의 사실 검증은 공개된 **벤치마크 데이터셋** 부재로 인해 거의 다루어지지 않았다.

대화에서 **사실 검증**이 어려운 핵심 원인은 대화체의 특수성이다. 대화에서의 발화는 뉴스나 위키 문장과 달리 구어체(일상 언어)로 이루어지며, **개인적 의견**이나 **감정 표현**, **속어(colloquialisms)** 등이 혼재되어 있어 무엇이 검증 가능한 **사실(claim)**인지 판별하기 어렵다. 또한 대화 문맥에는 앞서 언급된 대상을 가리키는 **지시어(coreference)**나 말해지지 않은 부분을 문맥으로 짐작하는 **생략(ellipsis)**이 빈번하여, 발화만 보면 불완전하거나 모호한 정보가 된다. 이러한 맥락 의존성 때문에, 대화 발화의 진위를 판단하려면 대화 흐름 전체를 이해하고 **외부 지식**과 대조해야 한다. 기존의 자동 사실 검증 모델들을 대화에 그대로 적용하면 성능 저하가 크다는 보고도 있다 (예: FEVER 데이터로 학습된 모델은 DialFact에서 저조한 성능을 보임)【(Gupta et al., 2022)】. 따라서 **대화 응답의 사실적 정확성(factual correctness)**을 평가하고자 하는 본 연구에서는, 현존 접근법의 한계를 극복하기 위해 새로운 데이터셋과 방법이 필요함을 정의하고 있다.

## 제안 방법
**DialFact**는 **대화 기반 사실 검증**을 위해 새롭게 구축한 벤치마크 데이터셋으로, 약 **22,245개**의 대화 응답 주장(claim)들을 그 근거 **위키피디아 증거 문장들**과 함께 제공한다 (각 주장마다 다수의 증거 문장 포함). 이 데이터셋을 통해 저자들은 대화 응답의 진위 판정 문제를 세 단계 서브태스크로 구성하였다: **(1) Verifiable claim detection** – 주어진 대화 응답이 검증 가능한 **사실 진술(Verifiable)**인지, 아니면 단순 의견/질문 등의 **비사실적 진술(Non-Verifiable)**인지 식별한다. **(2) Evidence retrieval** – 검증 가능하다고 판정된 경우, 외부 **지식 코퍼스(Wikipedia)**에서 관련 있는 **증거 문서와 문장**을 검색 및 추출한다. **(3) Claim verification** – 대화 **문맥(context)**과 수집된 증거를 바탕으로 해당 응답 주장이 **SUPPORTED(사실로 확인)**인지, **REFUTED(거짓으로 반증)**인지, 또는 **NEI (Not Enough Information, 정보 부족)**인지 판정한다. 이러한 3단계 구조는 Figure 1에 예시되어 있으며, 대화 응답을 사실 검증 **파이프라인**으로 다루는 본격적인 시도를 제안한다.

DialFact 데이터셋 구성에서 가장 중요한 점은 **대화 맥락**을 포함한 **사실형 주장**을 충분히 확보하는 것이다. 저자들은 **Wizard-of-Wikipedia (WoW)** 대화 데이터셋【(Dinan et al., 2019)】을 기반으로, 두 가지 경로로 다양한 유형의 주장들을 생성했다. 첫째, **자동 생성된 주장 (automatically generated claims)**으로서 **기계 학습 모델**과 **규칙 기반 변환**을 활용하여 새로운 발화를 만들어냈다. 구체적으로, 원본 WoW 대화에서 지식에 기반한 마법사 발화들을 변형하거나 새로운 증거로 **모델 생성 응답**을 만들어 **SUPPORTED/REFUTED/NEI** 세 가지 범주를 모두 포함하도록 했다. 변형 기법으로는 (a) **부정(Negation)**: Thorne 등(2019)의 42개 부정 변환 규칙을 적용해 원래 주장에 "not", "no" 등을 삽입함으로써 의미를 뒤집어 **REFUTED** 유형의 주장을 생성했다. (b) **치환(Substitution)**: **개체명 교체**를 통해 사실을 변경하는 방법으로, 스패시(Spacy) NER로 인식한 인물을 같은 대화 문맥이나 관련 위키 문서의 다른 인물로 바꾸거나, Sense2Vec 임베딩을 활용해 유사 의미 개체로 치환하거나, WordNet 반의어로 **형용사**를 반대 의미로 교체하는 등의 전략으로 사실을 틀리게 만들어 **REFUTED** 주장을 얻었다. (c) **마스킹 및 대치(Mask-and-Fill)**: 원래 발화의 핵심 단어들을 가리고, **T5** 언어모델【(Raffel et al., 2020)】을 fine-tuning하여 그 빈칸을 채우도록 함으로써 문장의 일부 내용을 변화시켰다. 저자들은 FEVER2.0 연구【(Hidey et al., 2020)】 및 Neutrality Masker 모델【(Shah et al., 2020)】를 활용해 어떤 단어를 가리면 SUPPORTED↦NEI로 뒤바뀔지 예측하고, 이를 기반으로 T5-base 모델을 WoW 데이터(증거 문장 + 문맥 → 마스킹된 응답 복원)로 학습시켜, **원래 증거와 불일치하는 세부정보**를 가진 응답을 생성했다. (d) **지식 기반 생성(Generation)**: Facebook **BlenderBot** 대화 생성 모델【(Roller et al., 2021)】를 WoW 대화에 fine-tuning하여, 다른 증거 조각들을 조건으로 대화 응답을 생성하도록 했다. 예를 들어 원 대화의 증거 대신 **무관하거나 임의의 위키 문장**을 넣어 대답을 만들면, 문맥과 겉보기에는 자연스럽지만 사실은 틀린 정보를 담은 발화가 만들어져 **REFUTED 또는 NEI** 사례를 추가로 확보할 수 있다. 이렇게 다양한 방식으로 한 대화 문맥당 다수 후보 응답 Rc를 생성한 뒤, **혼란도** 기반의 필터링을 적용했다. 즉, 최종 테스트 셋을 어렵고 다양하게 만들기 위해, 생성된 후보 중 문장이 서로 지나치게 유사한 것(중복 표현)이나 **유창성**이 떨어지는 것(GPT-2 언어모델 **Perplexity** 점수 기준 임계치 초과)을 제거했다. 그리고 Dialogue NLI【(Welleck et al., 2019)】, 대화 모순 검출 모델(DECODE, (Nie et al., 2021)), FEVER 검증 모델【(Schuster et al., 2021)】, 구어체 사실검증 모델【(Kim et al., 2021)】 등의 사전 학습된 모델들을 활용해 각 후보 발화에 대한 예측 **불확실성(entropy)**을 합산하여 정렬, **모델들이 가장 혼란스러워하는 상위 4개** 응답을 해당 문맥의 최종 후보로 선정했다. 이처럼 자동 생성 과정을 통해 **모델 입장에서 난이도가 높은** 다양한 거짓 및 불충분 주장들을 확보했다.

둘째 경로로, **사람이 작성한 주장 (human written claims)**을 추가하여 **자연스러운 대화체 표현**을 보강했다. Amazon Mechanical Turk를 통해, 작업자들이 주어진 대화 문맥과 증거 문장 세트를 보고 **사실적 주장** 문장을 새로 작성하도록 했다. 이때 작성할 문장의 레이블(SUPPORTED, REFUTED, NEI)을 미리 지정하여, 작업자들이 해당 증거에 부합하거나 모순되거나 정보 부족인 응답을 직접 만들도록 했다. (예: 증거를 읽고 “Refuted” 레이블이라면 증거와 모순되는 응답을 창작하되, 노골적인 부정어 사용을 피하도록 가이드함). 이렇게 1차로 작성된 응답들을 다시 앞의 **검증 인터페이스**에 넣어 3인 다수결로 레이블 검증을 거쳤고, 최초 의도한 레이블과 불일치하는 응답들은 수정 또는 제외하여 약 7%를 걸러냈다. 최종적으로 DialFact의 **검증용(validation)** 및 **테스트용(test)** 데이터셋에는 위의 **자동 생성**과 **인간 작성** 양쪽에서 얻은 **검증 가능 주장**들이 혼합되어 포함되며, 각 응답에 대해 **대화 맥락**, 관련 **증거 세트**, 그리고 **진실 여부 레이블**(Supported/Refuted/NEI)이 주어진다. 이렇게 구축된 DialFact는 **대화 맥락이 있는 사실 검증** 벤치마크로서 기존 FEVER 등과의 차별점을 갖고, 대화 응답의 사실성 평가를 위한 첫 공개 데이터이다.

마지막으로, 저자들은 해당 데이터셋에 대한 **모델링 방법**으로 **경량의 데이터 효율적 모델**을 제안하였다. DialFact는 **검증 전용 (testing benchmark)**으로 제작되었기에, 저자들은 **학습용으로 별도 생성한 약한 지도 학습 데이터**를 활용하여 모델을 훈련했다. 제안된 **Aug-WoW 모델**은 Wizard-of-Wikipedia 대화의 **훈련(split) 부분**에서 위와 유사한 기법들로 **대량의 합성 데이터**를 만들어 **BERT 기반 분류기**를 학습한 것이다. 구체적으로, Aug-WoW 모델은 BERT-base를 기반으로 하여 WoW로부터 생성한 **수십만 규모**의 (문맥, 증거, 주장) 삼쌍 데이터에 대해 **Supported/Refuted/NEI** 다중분류로 파인튜닝하였다. 학습 시 **대화 마지막 두 문장**과 **증거 문장들**을 [EOT] 토큰으로 구분해 입력으로 사용하고, 출력으로 3-way 레이블을 예측한다. 또한 **모델 성능 향상 전략**으로, FEVER의 증거-claim 대조 학습에 쓰인 **VitaminC** 코드베이스를 활용하고, **구어체 변환 데이터**【(Kim et al., 2021)】로 추가 튜닝한 **Colloquial 모델**과 비교하는 등 다양한 베이스라인을 함께 제시했다. 요약하면 **제안 방법의 핵심 아이디어**는, **대화형 지식 활용** 데이터(WoW)를 바탕으로 **사람 + 기계 생성**을 통해 **사실 검증 테스트셋**을 구축하고, 이를 평가하기 위한 **약지도 학습 기반 모델(Aug-WoW)**을 도입함으로써 **대화 도메인**의 사실 검증 성능을 크게 개선하는 것이다. 기존 방법들이 대화체를 다루지 못했던 한계를 넘어, 대화 속 사실 주장 검출부터 증거 검색, 검증까지 **엔드투엔드 평가체계**를 마련한 점에서 차별화된다.

## Figures
- **Figure 1:** DialFact에서 정의한 **대화 기반 사실 검증** 단계별 과정을 보여주는 예시이다. 상단에는 대화 문맥(Context)으로 A와 B의 짧은 대화가 주어지고, 그 아래 **Evidence**로 관련 위키피디아 문장 “Ireland is an island in the North Atlantic.”가 제시된다. 이때 B의 응답으로 네 가지 경우가 나올 수 있음을 보여준다: (1) **Non-Verifiable Response** – “I haven’t been but want to!”처럼 **검증 불가능한 개인적 발화**로, 사실 정보가 없으므로 검증 대상이 아니다. (2) **Verifiable Supported Response** – “It is an island in the north Atlantic right?”처럼 **검증 가능한 사실 진술**을 포함하며 **증거와 일치**하는 응답이다. (3) **Verifiable Refuted Response** – “Isn’t it somewhere in north Pacific?”처럼 **검증 가능하지만 증거와 모순되는** 응답이다. (4) **Verifiable NEI Response** – “I heard it’s the most popular tourist location in Europe!”처럼 **검증 가능한 주장이나, 제공된 증거만으로는 확인 불가(정보 부족)**한 응답이다. Figure 1은 이런 예시들을 통해, 먼저 **Verifiable 여부 판별 → 증거 수집 → 최종 레이블(Supported/Refuted/NEI) 판정**이라는 **파이프라인 구조**를 한눈에 나타낸다【(Gupta et al., 2022)】.

- **Figure 2:** 제안한 **Aug-WoW 모델**의 **혼동 행렬(confusion matrix)**이다. 세로축은 **실제 레이블**(Supported/Refuted/NEI), 가로축은 **모델 예측 레이블**을 나타내며, 각 칸의 수치는 DialFact 테스트셋에서 해당 분류로 분류된 수를 의미한다. 이 그림에서 주목할 점은 **NEI(정보 부족)** 클래스 관련 오류가 두드러진다는 것이다. Aug-WoW 모델은 NEI로 실제 분류되어야 할 사례 중 상당수를 **Refuted**로 잘못 예측하고(NEI 행의 Refuted 열), 또는 반대로 Refuted를 NEI로 착각하는 경우가 많다. 즉 **거짓 주장**과 **정보 부족** 상황을 혼동하는 경향이 가장 크며, 이는 **증거 부재**와 **증거 모순**의 구분이 모델에 어려운 과제임을 시사한다. 또한 **Supported**에 대해서는 비교적 혼동이 적지만, 여전히 일부 Refuted/NEI를 Supported로 잘못 판정하는 경우도 있어, 모델 정확도가 100%에 미치지 못함을 알 수 있다.

- **Figure 3:** **크라우드소싱 주석 인터페이스**의 스크린샷이다. 작업자들은 이와 같은 UI를 통해 대화 문맥, 해당 응답 주장, 그리고 관련 **후보 증거 문장들**을 한 번에 보게 된다. Figure 3에서 볼 수 있듯이, 인터페이스 상단에는 대화의 이전 턴들이 나열되고, 하단에는 미리 수집된 증거 문장들이 리스트업되어 있다. 작업자는 먼저 해당 응답이 **Verifiable**한지 **Non-Verifiable**한지 체크하고, 검증 가능하다면 제공된 증거 중 관련 있는 문장을 하나 이상 선택한다 (만약 제시된 증거만으로 부족하면 검색을 통해 추가 증거를 넣을 수도 있다). 마지막으로 그 증거에 비추어 **Supported / Refuted / NEI** 중 하나로 응답의 사실성 레이블을 할당한다. 이 인터페이스는 작업자가 증거를 검토하도록 유도하고, 만약 자동 생성 응답에 문법 오류나 문맥 부적절함이 있으면 편집할 수 있는 기능도 제공하였다. Figure 3은 이러한 **3단계 주석 절차**를 실제로 어떻게 진행했는지 보여주며, 이를 통해 데이터 품질을 높였음을 시각적으로 제시한다.

- **Table 1:** DialFact 데이터셋의 **통계량**을 정리한 표이다. **Validation(검증)**과 **Test(테스트)** 두 세트에 대해, **Supported / Refuted / NEI-Factual / NEI-Personal** 네 가지 범주의 **클레임 수**를 집계한다. 여기서 **NEI-Factual**은 **검증 가능하지만 정보 부족**으로 판정된 경우, **NEI-Personal**은 **검증 불가능한 개인적 진술**(Non-Verifiable)로 판정된 경우를 나타낸다. *Generated*와 *Written* 열은 해당 클레임이 **자동 생성**된 것인지 **인간 작성**된 것인지에 따른 분포를 보여준다. 표에 따르면, **Supported**와 **Refuted** 클래스는 각각 약 3,900여 개로 **균형있게** 포함되었다. NEI-Factual과 NEI-Personal도 합쳐 약 3,900여 개로, 전체적으로 3분할에 가까운 분포를 가진다. Validation 세트 총 10,436개, 테스트 세트 총 11,809개의 클레임이 있으며, 이 중 약 절반(테스트셋 기준 6,224개)은 **모델로부터 자동 생성**된 응답, 절반(5,585개)은 **사람이 작성**한 응답이다. 또한 **Non-Verifiable (NEI-Personal)** 사례의 대부분이 Generated 쪽에서 나오는 것을 볼 수 있는데 (예: 테스트셋 NEI-Personal 1,305건 중 1,221건이 기계 생성), 이는 **인간 작성** 단계에서는 주로 검증 가능한 사실 진술을 생성하도록 유도했기 때문이다. Table 1의 수치는 DialFact가 **다양한 출처**(사람/모델)와 **다양한 레이블**의 사례를 고르게 포함하며, 특히 대화 특유의 **개인적 진술(NEI-Personal)**도 상당수 포함하고 있음을 보여준다.

- **Table 2:** DialFact **Refuted 레이블**에서 자주 등장하는 **이항(bigram) 단어열**과 해당 레이블과의 **연관도**를 나타낸 표이다. 저자들은 Schuster 등(2019)의 방법에 따라 **LMI (Local Mutual Information)** 지표를 사용해 특정 이항이 Refuted 레이블에 얼마나 밀접히 연결되는지 계산했다. (LMI는 단어 w와 레이블 l의 동시확률 대비 조건부확률 편차를 나타내는 척도로, 수식은 $LMI(w,l) = p(w,l) \log \frac{p(l|w)}{p(l)}$이다.) Table 2의 좌측 “All” 컬럼은 전체 Refuted 데이터에서 가장 높은 LMI를 보인 상위 단어쌍이고, “Labelled”과 “Written”은 각각 자동생성과 인간작성 부분에 대해 별도로 집계한 결과다. **상위 bigram 예시**를 보면 “he was”, “singer songwriter”, “on spectrum” 등의 내용 단어들이 있고, 부정어인 “is not”, “does not” 등은 상위에 없다. 이는 곧 **DialFact의 거짓 주장 데이터가 특정 부정어 패턴에 치우치지 않고** 다양한 내용으로 구성되었음을 의미한다. (참고로 FEVER 등의 경우 “not”이 포함되면 Refuted일 가능성이 높다는 편향이 있었다.) DialFact에서는 이러한 노골적 단서들을 없앰으로써 **모델이 의미적 판단을 하도록** 유도했다는 것을 Table 2가 보여준다.

- **Table 3:** **Verifiable Claim Detection** (검증가능 응답 식별) 태스크에 대한 **베이스라인 성능**을 나타낸 표이다. 이 태스크에서는 응답을 **Verifiable** vs **Non-Verifiable**로 이진분류하며, 표에는 **Accuracy**(전체 정확도)와 **Verifiable F1**, **Non-Verifiable F1** 점수가 나와 있다. 비교한 기법으로는 (a) **Random**: 무작위 예측 (기준선), (b) **Lexical**: 응답과 증거 문장 사이의 **최대 단어 중복률**로 판정, (c) **DNLI**: 대화 NLI 모델【(Welleck et al., 2019)】을 이용하여 중립 클래스의 확률이 0.5 이상이면 비사실로 보는 방법, (d) **Lexical+DNLI**: 전술한 두 방법을 조합한 모델이다. Table 3 결과에 따르면, Lexical 방법만으로도 정확도 79.4%로 높은 편이며, Dialogue NLI 기반 분류는 82.1%, 둘을 결합하면 **82.8%** 정확도로 가장 우수했다. 하지만 Non-Verifiable F1의 경우 최고 모델도 **39.1**에 불과하여, **개인적 발화 식별이 어려움**을 보여준다. Verifiable 클래스의 F1은 90 이상으로 양호하나, Non-Verifiable은 모든 모델에서 F1 30대에 머물렀다. 이는 **일부 의견성 발화**가 애매하거나, 발화 내에 사실 정보도 섞여 있어 모델이 혼동하기 때문으로 분석된다.

- **Table 4:** **문서 검색(Document Retrieval)** 단계의 성능을 나타낸 표로, **Recall@5** 지표로 측정하였다. 두 가지 검색 기법을 비교하는데, **WikiAPI**는 위키백과 API를 통해 질의 생성 및 문서 랭킹(BM25 등)으로 상위 문서를 찾는 전통 방식이고, **DPR (Dense Passage Retrieval)**는 미리 임베딩된 문서들에 쿼리 임베딩을 매칭하는 **BERT 기반** 기법【(Karpukhin et al., 2020)】이다. 또한 각 기법에 대해 **대화 문맥 사용 여부**에 따라 변형하였다: WikiAPI-ctx와 DPR-WoWft-ctx는 **질의에 대화의 마지막 두 발화까지 포함**한 경우이며, 대응하여 문맥 미사용 버전(WikiAPI*, DPR*)도 비교되었다. Table 4에 나타난 결과는 **대화 문맥을 포함한 검색 쪽이 항상 높은 Recall**을 보이며, **WikiAPI 계열**이 DPR보다 우세하다는 점이다. 예를 들어 WikiAPI-ctx의 Recall@5가 DPR-ctx보다 높았고, 문맥을 뺐을 때도 WikiAPI가 더 낫다. 이는 **대화 문맥의 키워드 활용**이 검색에 도움을 주고, **dense retrieval** 방식은 대화처럼 다양하게 표현된 질의엔 상대적으로 약할 수 있음을 시사한다. 또한 DPR의 경우 문맥을 넣으면 어느 정도 향상되지만, WikiAPI처럼 **엔티티 중심**의 정확한 문서를 찾는 능력은 다소 떨어져, 전체적으로 **WikiAPI+문맥**이 가장 효과적인 문서 검색 세팅으로 나타났다.

- **Table 5:** **증거 문장 검색(Evidence Sentence Selection)** 결과를 보여준다. 문서 검색 이후 실제 판정에 사용할 **개별 문장**을 추출하는 단계로, 여기서는 **Recall@5**로 **정답 증거 문장**을 상위 5개에 포함시켰는지를 측정했다. Table 5에서는 WikiAPI와 DPR 기반 방법 각각에 대해 **-ctx (문맥 포함)** vs **기본** 버전을 비교하고 있다. 결과는 **Table 4와 유사한 경향**으로, **대화 문맥을 활용한 경우**에 증거 문장 Recall이 더 높게 나타났다. 특히 WikiAPI-ctx는 가장 높은 성능을 보이며, DPR도 문맥 포함 시 다소 개선되었다. 이는 **대화 흐름의 정보를 쿼리에 넣는 것**이 어떤 발화에 증거가 필요한지 더 잘 짚어내는 데 기여함을 보여준다. 또한 WikiAPI 방식은 DPR보다 전반적으로 높은 Recall을 시현하여, **사전 구축된 dense index보다 실시간 검색이 이 경우 효과적**임을 나타냈다. 종합하면 Table 4, 5는 **증거 검색 단계**가 전체 fact-checking 성능에 매우 중요하며, **엔티티 중심 검색 + 문맥 활용**이 최적임을 보여준다.

- **Table 6:** **Claim Verification (사실 판정)** 태스크의 **테스트셋 성능 비교**를 제시한 핵심 결과 표이다. 여기서는 서로 다른 **증거 사용 설정** 하에서 여러 모델들의 **Accuracy와 Macro F1**을 비교한다. 증거 설정은 (i) **Oracle-Evidence**: 인간이 선택한 **정답 증거** 사용, (ii) **Wiki-Evidence**: WikiAPI-ctx + 후처리로 얻은 **시스템 증거**, (iii) **DPR-Evidence**: DPR-WoWft-ctx로 얻은 **시스템 증거**를 사용한 경우이다. 비교 모델들은 **DNLI**(대화 NLI 기본모델), **DECODE**(대화 모순판별 모델), **VitaminC**(ALBERT-base로 FEVER+비타민C 데이터 학습), **CorefBERT-Colloquial**(corefBERT 기반 구어체 검증 모델), **Colloquial**(BERT-base로 Kim 등(2021) 구어체 데이터 학습), 그리고 **Aug-WoW**(제안 모델)이다. Table 6 결과에서 **Aug-WoW 모델의 우수성**이 두드러지는데, 모든 증거 설정에서 Aug-WoW의 **정확도 ~51-69%**로 타 모델 대비 가장 높다. 특히 **Oracle 증거**가 주어진 이상적 상황에서 Aug-WoW 정확도 **69.2%**, Macro F1 **69.0**으로, 두 번째로 좋은 Colloquial 모델(63.5%, 62.8)보다 훨씬 높다. **Wiki-Evidence**나 **DPR-Evidence** 처럼 **실제 검색 증거**를 쓸 경우 모든 모델에서 성능이 떨어지지만, 그 격차가 Aug-WoW는 상대적으로 적어 **견고함**을 보인다. 예컨대 Aug-WoW Accuracy는 Oracle→Wiki 증거로 약 17.6포인트 하락했는데, VitaminC나 CorefBERT-Colloquial 등은 10포인트 이상 더 큰 하락폭을 보였다. 이는 **증거 추출 오차**에 대한 강인성이 Aug-WoW가 높음을 의미한다. 또한 **어느 설정에서도 70% 이상의 정확도를 보인 모델은 없고**, Oracle 증거 하에서도 최고 69% 수준이어서, **문제의 난이도**가 여전히 높음을 알 수 있다. Table 6은 결국 **제안 모델이 기존 방법들보다 이 대화 검증 과제에 적합**하며, 동시에 **retrieval** 단계의 개선 여지가 전체 성능 향상에 중요함을 시사한다【(Gupta et al., 2022)】.

- **Table 7:** **Aug-WoW 모델의 Ablation(구성 요소별 성능)** 실험 결과이다. 여기서는 **대화 문맥 사용 유무**와 **사전언어모델 규모**의 영향 두 가지를 살펴보았다. **Aug-WoW-noctx**는 응답 검증 시 **대화 문맥을 입력에 포함하지 않은** 모델이고, **Aug-WoW-BERTLarge**는 기본 BERT-base 대신 **BERT-large**를 사용한 모델이다. Table 7에서 Oracle/Wiki/DPR 각 증거 설정별 성능을 보면, **noctx의 경우 기본 Aug-WoW 대비 Oracle 증거 정확도는 69.2→68.1로 소폭 하락**했고, Wiki/DPR 증거 환경에서는 오히려 약간 높거나 비슷한 수치를 보였다(Macro F1 기준 51대 52%). 이는 **대화 맥락 정보가 없더라도** claim과 증거만으로도 어느 정도 판단 가능함을 의미하지만, 완전히 동일한 수준은 아니어서 **문맥이 주는 미세한 도움**은 있는 것으로 해석된다. 반면 **BERTLarge 버전**은 Oracle 증거일 때 Accuracy 70.9%로 base보다 높았으나, Wiki/DPR 증거일 땐 45.8%, 43.5%로 크게 낮아졌다. 즉 대용량 모델이 **깨끗한 증거 환경에서는 정확도 향상을 주지만**, **증거 노이즈가 있을 땐 오히려 민감하게 성능이 저하**되는 현상이 관찰되었다. 저자들은 BERT-large 모델이 작은 오류에도 과적합되거나 편향되는 경향을 보인다고 분석했다. 종합하면 Table 7은 **대화 문맥이 있으면 미세한 성능 향상에 기여하지만 결정적이진 않고**, **모델 크기 증대만으로는 일관된 개선을 보장하지 않는다**는 사실을 보여준다.

- **Table 8:** DialFact 테스트셋을 **응답 출처별(Generated vs Written)**로 나누어 **사실 검증 성능**을 평가한 표이다. 여기서는 **Oracle 증거**를 사용하여, 자동 생성된 응답들과 사람이 쓴 응답들 각각에 대해 모델들의 Accuracy와 Macro F1을 비교했다. 결과를 보면 대부분의 모델에서 **인간 작성 응답**에 대한 성능이 **기계 생성 응답**보다 높다. 예를 들어 **VitaminC 모델**은 Written에서 Accuracy 60.8%로 Generated에서의 48.9%보다 크게 높고, **Colloquial 모델**도 Written 64.7% vs Generated 61.3%로 차이가 있다. **Aug-WoW 모델**은 Generated 63.9%, Written 74.2%로 **약 10포인트 이상** 차이가 나며, **모든 모델 중 최고 성능**을 내긴 했지만 여전히 **generated 쪽에서 어려움**을 드러낸다. 특이하게 **DNLI**나 **DECODE** 모델은 Generated에서 오히려 높은 경우도 있는데, 이는 해당 모델들이 대화 생성 모델이 만든 어색한 문장 패턴에 과적합되었을 가능성을 시사한다. 전반적으로 Table 8은 **DialFact의 자동 생성 클레임들이 실제 사람이 쓴 문장보다 판별이 어렵고 더 함정이 많도록 설계되었음**을 뒷받침하며, Aug-WoW조차 human-written 응답에서의 성능 향상이 더 큼을 보여준다. 이는 **모델들이 자연스러운 표현보다는 기계적 패턴에 더 익숙**함을 의미하기도 하여, 향후 **모델의 대화 적응**에 고려할 점을 시사한다.

- **Table 9:** DialFact 데이터와 모델 예측의 **예시(case study)**들을 담은 표이다. 여러 대화 **Context**와 그에 대한 **Response(모델 출력)**, **Evidence**, 그리고 다양한 모델들의 예측 결과를 열거하여, **오류 양상**을 분석하였다. 표의 각 행은 하나의 사례로, 응답이 **Generated**인지 **Written**인지도 표기되어 있다. 예를 들어 한 사례에서 문맥이 “Biathlon에는 두 가지 스포츠 – 크로스 컨트리 스키와 사격 – 가 결합되어 있어요”이고 응답이 “그건 cross country skiing이 아니라 track sport죠”라는 **잘못된 주장**인 경우를 보자. 증거로는 바이애슬론이 스키+사격이라는 설명이 제시되었다. 이 사례에서 **DNLI, CorefBERT-Colloquial, DECODE** 등 몇몇 모델은 이를 **Supported**로 잘못 판단했다 (S), 반면 **Colloquial, Aug-WoW** 등은 **Refuted**로 옳게 판단했다 (R). 이 예시는 **응답과 증거 사이 일부 단어 겹침**(예: “cross country”) 때문에 단순 모델들이 착각하여 Supported로 분류한 것을 보여준다. 또 다른 사례에서는 “Elvis Presley가 1977년에 죽었다”는 문맥에 대해 응답이 “그의 사망일을 기억 안나지만 러시아에서는 아직도 살아있다고 생각해요”라는 **황당한 주장(거짓)**이 있다. 증거로 엘비스 사망일과 업적이 주어졌을 때, 대부분의 모델이 이것을 **NEI**나 **Refuted**로 적절히 거르는 반면, 어떤 모델은 **Supported**로 잘못 레이블했다. 이는 **“not one of his best groups” 등의 부분 문장**을 모델이 오해하거나, **문맥상의 부정 표현**을 잘못 처리한 오류로 보인다. Table 9 전반의 사례들은 모델들이 **(a) 증거-주장 간 단순 어휘 매칭에 의존**하여 잘못된 지원 판정을 내리거나, **(b) 상식적 추론이 필요한 부분에서 실패**하거나, **(c) 부정어 등 표현에 민감하게 속아** 잘못 예측하는 현상이 있음을 보여준다. 이 사례 분석을 통해 **모델의 한계**를 짚고, 어떤 유형의 개선이 필요한지 논의하고 있다.

- **Table 10:** **Validation 세트**에서의 claim verification 결과 표로, 구조는 Table 6과 유사하다. 주요 추세는 테스트셋과 비슷하여, **Aug-WoW가 모든 증거 설정에서 최고 성능**이며, Colloquial, CorefBERT-colloquial 등이 그 뒤를 잇는다. 수치적으로 Validation에서는 Aug-WoW Accuracy가 Oracle 증거시 70.4%, Wiki 증거 51.2%, DPR 증거 50.4% 등으로 테스트셋 때와 큰 차이가 없다. 이는 **데이터 분포가 검증/테스트 일관적**임을 의미한다. Table 10을 통해 모델 선택 등 튜닝에도 활용했음을 엿볼 수 있다.

- **Table 11:** **비검증 발화 제거** 조건에서의 테스트셋 성능이다. DialFact의 NEI에는 **검증 불가(NEI-Personal)**와 **정보 부족(NEI-Factual)**이 섞여 있는데, 여기서는 Non-Verifiable인 NEI-Personal 사례들을 아예 제외하고 **오직 Verifiable claims만으로 3분류**한 경우를 실험했다. Table 11에 따르면, **모델들의 성능 향상이 소폭 있었다**. 예를 들어 Aug-WoW Accuracy가 69.7%로 기존 69.2%에서 약간 오르고, Macro F1도 Verifiable만 남기니 51.7%로 기존보다 0.2%p 상승했다. 다른 모델들도 전반적으로 약간씩 향상된 수치를 보였다. **전체 순위**나 경향은 Table 6과 거의 동일하게 유지된다. 이는 **비검증(개인적) 발화가 포함되어 있었던 것이 일부 모델에 혼선을 주었으나, 그 영향을 제거해도 근본적인 모델 간 성능 차이는 비슷함**을 보여준다. 요컨대 **Non-Verifiable 응답을 사전에 걸러내고 검증 작업을 하면** 절대 성능은 약간 올라가지만, **여전히 해결해야 할 도전(Refuted vs NEI 구분 등)은 남아 있다**는 것이다.

- **Table 12:** **이진 분류** 설정에서의 테스트셋 성능이다. 여기서는 **SUPPORTED vs NOT-SUPPORTED** (REFUTED+NEI를 통합)로 단순화하여, 사실 여부만 판단하도록 한 실험 결과를 보여준다. Table 12에 따르면, **모든 모델의 정확도가 크게 향상**된다. Aug-WoW는 Accuracy **80.6%**, Macro F1 78.8%로 3분류 때보다 약 11포인트 상승했고, 다른 모델들도 70% 이상까지 올라왔다. 즉 **거짓과 정보부족을 한 범주로 묶으면** 모델들이 훨씬 수월하게 판단함을 알 수 있다. 특히 Colloquial 모델은 76.8%까지 올라 Aug-WoW와의 격차가 좀 줄었지만, 여전히 Aug-WoW가 최고다. 이 결과는 **지원 vs 비지원만 판단하는 건 비교적 쉽지만**, 세분화된 **거짓 vs 정보부족 구분이 어려워 성능을 떨어뜨린다**는 점을 보여준다. 따라서 향후 해당 세분화에 대한 모델 개선이 필요함을 시사한다.

- **Table 13:** DialFact 데이터셋에서 뽑은 **예시 대화** 두 건을 상세히 보여주는 표이다. 상단 “Example 1”은 **자동 생성된 응답**의 사례로, 컨텍스트와 증거, 응답, 그리고 정답 레이블 및 설명이 수록되어 있다. 예를 들어 컨텍스트가 “I think Jazz is an American creation!”이고 증거로 “Jazz has roots in West African... and European military band music”가 주어졌다면, 응답으로 “Its roots include African-American music traditions including blues and ragtime” 같은 문장이 나올 수 있다. 이 응답은 **증거로부터 검증 가능한 사실 진술(SUPPORTED)**로 레이블되며, Explanation에 해당 증거로 검증 가능함이 적혀 있다. 하단 “Example 2”는 **인간 작성 응답** 사례로, Elvis Presley의 사망 관련 대화에서 응답들이 어떻게 레이블되고 증거와 연결되는지 보여준다. Table 13은 이러한 **실제 데이터 사례**를 통해 DialFact의 어노테이션이 어떤 형태로 이루어졌는지 감을 주며, 모델의 입력-출력 관계를 이해하는데 도움을 준다.

## 실험 환경
**데이터 및 평가 구성:** DialFact는 훈련 세트 없이 검증/테스트 셋만 제공되므로, 저자들은 **외부 데이터**를 활용하거나 **합성 데이터**를 만들어 모델을 훈련했다. Aug-WoW 모델 학습에는 **Wizard-of-Wikipedia의 훈련 split**에서 생성한 **약 100k+ 규모**의 (문맥, 증거, 주장, 레이블) 데이터가 사용되었으며, FEVER 등 기존 데이터도 간접 활용되었다(예: VitaminC의 사전학습 모델 활용). **검증/테스트 평가지**로는 Accuracy와 Macro F1 (3-class 균형 성능)을 주요 지표로 삼았다. Verifiable 여부 검출에는 Accuracy와 클래스별 F1, 증거 검색에는 Recall@K (주로 @5)로 평가했다. 

**모델 및 구현:** Claim verification을 위한 분류 모델들은 모두 **프리트레인 언어모델** 기반으로 구현되었다. **BERT-base**를 기본으로 하여, **Aug-WoW** 및 **Colloquial** 모델을 학습했고, **BERT-large**는 ablation에 사용했다. **CorefBERT** 모델의 경우 공개된 CorefBERT-base를 불러와 FEVER 구어체 데이터로 파인튜닝했다. **VitaminC baseline**은 Tal Schuster 등이 공개한 **ALBERT-base-vitaminC-fever** 모델 체크포인트를 사용했다. 이러한 모델들의 학습과 추론은 파이토치(PyTorch) 기반으로 진행되었으며, 특히 **VitaminC**의 공식 깃허브 코드를 활용하여 Aug-WoW 및 Colloquial 모델도 같은 파이프라인으로 훈련했다【(Schuster et al., 2021)】. **대화 모순 검출**인 DECODE 모델도 BERT-base로 WoW 대화의 contradiction vs not 데이터(약 310k 샘플)로 재학습했다.

**검색 모듈:** 증거 검색을 위해 **MediaWiki API**와 **BM25** 랭킹을 이용해 위키 문서를 검색했고, 문장 선택에는 **SpaCy**의 word2vec 유사도와 **BM25 점수**를 조합하여 상위 문장들을 추렸다. 또한 Dense Retrieval 비교를 위해 Facebook **DPR** 구현체를 활용하였다. DPR 모델은 **BERT-base 쿼리/문서 인코더**로, WoW 대화 데이터로 **파인튜닝(DPR-WoWft)**하여 사용하였다. Entity linking에는 KILT 툴킷 등을 참고했으며, NER 태그 추출에 SpaCy, Sense2Vec(Trask et al., 2015) 등을 사용하여 질의 확장을 진행했다.

**어노테이션 환경:** Amazon **Mechanical Turk** 플랫폼에서 미국 기반 작업자를 대상으로 3회에 걸쳐 Annotation을 실시했다. **퀄리티 컨트롤**을 위해 HIT 5000건 이상, 승인률 95% 이상의 숙련된 작업자만 선발했고, 사전에 **자격 시험**(예제 12건 레이블링)으로 87명의 작업자를 통과시켰다. 인터페이스(앞의 Figure 3)에는 **자동 검증 스크립트**를 내장하여, 작업자가 증거를 선택하지 않았거나 응답을 복붙한 경우 경고를 주고, 부적절한 주석을 다는 작업자는 재교육 또는 배제하였다. 이러한 관리 하에 총 12,000여 건(검증+시험 데이터 합산)의 응답에 대해 **크라우드 소싱 라벨**을 얻었으며, **추가 교차 검증**으로 10% 샘플에 대해 중복 라벨을 받아 **Krippendorff’s α** 값을 측정했다. 그 결과 **카테고리 레이블**에 대해 인간 작성 응답은 α=0.68, 기계 생성 응답은 α=0.58 정도로 **보통 수준의 합의도**를 보였다 (특히 Verifiable vs Non-Verifiable 구분 α=0.49로 모호한 사례가 있었음). 이는 앞서 언급한 **개인 의견 vs 사실 여부 판단의 어려움**을 방증한다.

**하드웨어 및 효율:** 모델 학습은 NVIDIA **A100 GPU 4장**이 장착된 서버에서 병렬 수행되었다. Aug-WoW와 Colloquial 모델은 배치 크기 100으로 학습했고, 검증셋 성능 기준으로 3epoch 이내에서 Early Stopping으로 선택되었다. 증거 검색은 사전 인덱싱된 위키 문서(DB dump 사용)와 실시간 API 호출을 병행하였고, 이를 위해 검색 과정은 멀티스레딩으로 최적화했다. 전체 파이프라인(검증 여부 판단→문서/문장 검색→사실 검증)을 구성하여 실험했으며, 한 샘플 처리에 수 초 이내의 시간이 걸렸다. 따라서 DialFact 평가에는 다소 시간이 소요되나(증거 검색 단계), 연구목적 벤치마크로는 충분히 활용 가능하도록 구현되었다.

## 실험 결과
**Verifiable 주장 검출:** Table 3에서 보았듯, **Lexical+DNLI** 조합이 Accuracy 82.8%로 가장 우수했고, **DNLI 단독**도 82.1%로 근접했다. 하지만 **Non-Verifiable** 클래스에 대한 재현율은 매우 낮았다(F1≈0.37–0.39). 이는 특히 “최고의 록밴드는 Guns N’ Roses였다”와 같은 문장이 **개인 의견인지 사실인지** 애매하여 작업자 간에도 혼동을 준 사례들이 있었고, 대부분 이런 경우 안전하게 **NEI**로 처리되었음을 의미한다. 결국 모델들도 이러한 **경계 사례**를 확실히 구별하진 못했다. 그럼에도 전반적인 정확도가 80%를 넘는 것은, **대화 발화 중 상당수는 명백히 사실 정보 포함 여부가 갈리기 때문**으로 보인다 (예: “난 아일랜드 가봤어” vs “아일랜드는 ~~이다”는 쉽게 구분 가능). **추가 실험**으로 비검증 응답(개인적 발화)을 사전에 human oracle로 필터링할 경우, 이후 증거 검색과 검증 단계의 부담이 줄어들어 최종 성능이 다소 향상됨을 Table 11에서 확인했다. 이는 **해당 전처리**가 시스템 개선에 유용함을 시사하며, 실제 어플리케이션에서는 우선 **소형 분류기**로 verifiable 여부를 거른 뒤 남은 것만 복잡한 검증을 하는 방식이 효율적일 수 있음을 보여준다.

**증거 검색 성능:** Table 4, 5에서 드러나듯, **대화 문맥 포함** 여부가 검색 성능에 영향을 주었다. 문맥을 포함한 쿼리는 그렇지 않은 쿼리 대비 문서 Recall@5 기준 약 +5~10%p 향상을 보였다. 이는 대화 마지막 발화뿐 아니라 이전 화자 질문 등 **맥락 키워드**가 증거 문서 연결에 도움을 주는 것으로 풀이된다. 또한 WikiAPI(BM25) vs DPR 비교에서는 **WikiAPI** 쪽이 우세했는데, 이는 **엔티티명 정확도**에서 기인한 것으로 보인다. 예컨대 대화에서 특정 인물이나 작품명이 나오면 WikiAPI는 해당 문서를 잘 찾아내지만, DPR은 훈련 코퍼스에 없는 고유명사 표현에 약해 찾지 못하는 경우가 있었다. 반면 DPR은 문맥이 아예 일치하지 않아도 의미적으로 근접한 문서를 찾는 능력이 있어, WikiAPI가 놓친 **동의어 표현** 등을 일부 보완했다. **최종 증거 선택** 단계에선 두 접근을 조합하여 unique한 문장들을 모아 candidate set을 만들었고, crowd worker들이 필요한 경우 추가 증거를 수동 검색하도록 해 **recall을 최대화**하였다. 그 결과 최종 DialFact 데이터에서는 각 claim당 평균 1.3개의 증거 문장이 확보되었으며, 일부 NEI의 경우 관련도 높은 문장(정답은 아니지만 토픽상 연관된 문장)들이 추가되어 있다. 

**Claim Verification 성능:** 주요 결과로, **Aug-WoW 모델**이 전 영역에서 가장 뛰어난 성능을 기록했다. **FEVER 등 기존 데이터로 학습된** VitaminC, Colloquial, CorefBERT 등은 대화체에 어느 정도 대응하나, Aug-WoW처럼 **대화 데이터로 직접 약지도 학습**한 경우를 따라오진 못했다. 특히 **Oracle 증거** 상황에서 Aug-WoW 정확도가 69.2%인데 비해, 차선인 Colloquial은 63.5%, VitaminC는 57.6%에 그쳤다. 이는 **대화 맥락의 colloquialism, coreference 등을 충분히 반영한 학습**이 중요함을 보여준다. 반면 **retrieved 증거 사용 시 모든 모델 성능이 감소**한 점도 주목된다 (예: Aug-WoW Accuracy Oracle→Wiki 증거: 69.2→51.6%). 이는 잘못 검색된 증거나 불완전한 증거로 인해 **검증 단계 오류**가 생기기 때문이다. 특히 **Refuted나 NEI 판단**에서는 관련 없는 문장이 섞이면 모델이 판단을 유보하거나 잘못할 확률이 높아진다. 이러한 현상은 **pipeline 구성**의 한계로, **end-to-end**로 joint training하거나 **evidence reranking** 등의 개선 여지가 있다.

**Aug-WoW 상세 분석:** Ablation 실험 결과, **대화 문맥**을 입력에 포함하지 않아도 성능이 크게 떨어지지 않았다. 이는 많은 경우 응답 문장 자체와 증거로 판단이 가능함을 뜻한다. 예컨대 응답: “그는 1988년에 사망했어” & 증거: “Elvis Presley died 1977”이면, 문맥 없이도 모순임을 알 수 있다. 다만 문맥이 없는 모델은 **증거와 claim 간 단순 매칭에 더 의존**할 수 있어, 일부 NEI vs Refuted 구분에서 혼동을 늘릴 위험이 있다. BERT-large로 모델 크기를 키운 경우, Oracle 증거 상황에선 미세한 성능 향상이 있었으나, 노이즈 존재 시 성능이 불안정해졌다. 이는 **작은 학습세트**에 큰 모델을 쓰면 과적합이나 불안정성이 증가할 수 있음을 시사한다. 따라서 **현 단계 최고 성능**은 BERT-base 규모의 Aug-WoW로 얻었으며, 이는 데이터 효율적 접근의 유효성을 보여준다.

**오류 및 한계 분석:** Table 9 등에서 추출한 사례들을 통해 발견한 **모델 오류 패턴**은 다음과 같다:
- **단어 중첩 편향:** 응답과 증거에 **겹치는 단어**가 많으면 (비록 의미는 다르더라도) 모델이 **Supported**로 오인하는 경향이 있었다. 예: “biathlon” 문맥에서 “cross country”라는 단어가 응답과 증거에 모두 나오자, 실제로는 잘못된 주장인데도 Supported로 예측한 모델들이 있었다. 이는 **피상적 키워드 매칭**에 의존하는 한계를 보인다.
- **상식/추론 부족:** **증거와 직접 충돌하지 않지만 사실상 거짓**인 주장을 모델이 잡아내지 못하는 경우. 예를 들어 “Most people in Russia see him as alive”라는 응답은 증거 상 직접 모순은 없으나,常識적으로 엘비스는 사망한 사실이 알려져 있어 **사실상 거짓**이다. 이런 **망상적 주장**에 대해 몇몇 모델은 NEI로 두기도 했는데, 이는 **텍스트 외 지식 활용**이나 **복합 추론** 능력이 부족함을 보여준다.
- **부정어 처리:** 응답이나 증거에 **“not”, “no”** 같은 부정어가 등장하는 경우 모델이 **Refuted로 치우치거나**, 반대로 **부정어를 무시**하고 잘못된 Supported 판단을 내리는 일이 있었다. 이는 **미세한 어조 변화**를 잡는 NLP의 어려움을 반영한다. DialFact 데이터 자체는 이런 단순 단서에 편향되지 않게 설계되었으나, 모델은 여전히 완벽히 극복하지 못했다.
- **Verifiable 판단의 애매함:** 일부 문장은 사실 정보처럼 보이지만 **평가자에 따라 의견으로 볼 수도 있는 회색 지대**가 있었다. 예: “Guns N’ Roses was the greatest rock band of all time”은 일부는 사실처럼, 일부는 주관적 표현으로 볼 수 있다. 이러한 경우 라벨링 합의도 어려웠고, 결국 **NEI로 처리**되었는데, 모델도 종종 혼란스러워한다. 이는 **사실/의견 경계 학습**이라는 별도의 난제가 존재함을 나타낸다.

**전반적 요약:** 제안한 Aug-WoW 모델은 이전 방법들보다 대화체 사실 검증 성능을 향상시켰지만, **약 70% 수준 정확도**로 아직 충분치 않다. 특히 **Refuted vs NEI 구분**, **개인 의견 식별**, **증거 검색 오류 대응** 등 여러 측면에서 챌린지가 확인되었다. 저자들은 추가로 **2-way 분류**(Table 12)를 해보니 80% 이상 정확도가 나온 것을 들어, **더 세밀한 분류가 어렵지만 필요한 영역**이라고 강조한다. 또한 이러한 분석을 통해 **향후 연구 방향**으로 **coreference 해소**, **대화 맥락 이해 향상**, **더 정교한 retrieval** 등이 필요함을 언급했다. 결과적으로 DialFact를 통해 **기존 fact-checking 모델들이 대화 도메인에서 성능 부족**함이 드러났고, **새로운 데이터와 접근법으로 어느 정도 개선**은 했지만 **여전히 개선 여지**가 많다는 것이 실험을 통해 입증되었다.

## 배경지식 및 핵심 용어
- **Fact-checking (사실 검증):** 주어진 주장(claim)이 진실인지 거짓인지, 혹은 정보 부족인지를 **권위 있는 증거에 비추어 확인**하는 작업. 일반적으로 (claim, evidence) 쌍을 분류하는 문제로 정의되며, FEVER 등에서는 위키피디아 문서를 증거로 사용했다.
- **FEVER:** “Fact Extraction and VERification” 벤치마크 데이터셋 【(Thorne et al., 2018)】. 위키백과 문장에서 추출/편집된 **지원 또는 반박 문장**과 인위 조작된 거짓 주장들로 구성되어, claim을 Supported/Refuted/NotEnoughInfo로 분류하는 과제로 널리 사용됨. 대화 문맥은 없고, 단문 위주의 사실 검증 데이터.
- **Wizard-of-Wikipedia (WoW):** 오픈도메인 **지식 대화** 데이터셋 【(Dinan et al., 2019)】. 위키피디아 문서를 보며 대답하는 “마법사”와 일반 사용자인 “견습생” 간의 대화 22k여 개로 구성. 각 턴마다 마법사는 위키 문장 중 하나를 선택해 활용하도록 설계되었음. DialFact는 이 WoW 대화의 **검증 가능한 발화들**을 활용하고, 추가로 거짓 주장들을 생성해 데이터셋을 만들었다.
- **Verifiable vs. Non-Verifiable Claim:** **Verifiable**한 주장이란 *객관적 사실 정보*를 담고 있어 권위있는 배경지식(Wikipedia 등)으로 **검증될 수 있는** 발화이다. **Non-Verifiable**은 *개인의 감정, 바람, 질문, 취향* 등 **객관 검증 불가한 내용**으로 이루어진 발화이다. DialFact에서는 Non-Verifiable 응답은 자동으로 NEI(Not Enough Info)로 레이블된다.
- **Supported / Refuted / NEI:** 검증 가능한 주장에 대해 부여되는 **사실 검증 레이블 3종**. **Supported**는 증거로 뒷받침되는 참, **Refuted**는 증거와 모순되는 거짓, **NEI (정보 부족)**는 증거를 충분히 찾을 수 없어 진위 판단이 불가한 경우를 뜻한다. (Non-Verifiable한 경우도 편의상 NEI로 처리하지만 DialFact 통계에서는 별도로 구분함.)
- **DNLI:** **Dialogue Natural Language Inference** 데이터셋 【(Welleck et al., 2019)】. 일상 대화에서 도출한 추론 과제로, 대화 응답이 이전 발화에 대해 긍정(entail), 부정(contradict), 중립(neutral) 관계인지 판단하도록 구성됨. 본 논문에서는 DNLI의 **중립 클래스 확률**을 Non-Verifiable 추론에 활용했다.
- **DECODE:** **Dialogue Contradiction Detection** 모델 (Nie et al., 2021)로, WoW 등 대화 데이터에서 문맥 상 모순 여부를 탐지하는 알고리즘. FEVER 등의 외부 지식 없이 대화 내 전후 발화만 보고 모순을 판단하도록 훈련됨. 본 연구에서 DECODE는 claim과 직전 문맥을 입력으로 Refuted 여부를 예측하는 baseline으로 사용되었다.
- **VitaminC:** Tal Schuster 등【(2021)】이 제안한 **대규모 대조학습 데이터**로, 위키 문장을 다양한 방법(negation, paraphrase 등)으로 변형해 약 400k 쌍의 (지원문장, 변형문장) 및 레이블을 생성한 것. 사실 검증의 **robustness** 향상을 위해 고안된 데이터셋이며, 알버트(Albert) 등 모델을 학습시키는 데 활용된다. 본 논문에서는 VitaminC로 학습된 ALBERT 모델을 baseline으로 쓰고, Aug-WoW 학습에도 해당 코드베이스를 참조했다.
- **Colloquial Claims (구어체 주장):** Kim 등【(2021)】이 FEVER의 문어체 claim들을 **대화체 스타일**로 변환하여 만든 데이터. FEVER 문장을 인칭 대명사, 구어적 표현 등으로 바꾸고, coreference 등을 추가하여 더 **일상적인 어투**로 재작성한 후 사실 검증 라벨을 유지했다. 이 데이터를 BERT-base로 학습한 **Colloquial 모델**과, 여기에 CorefBERT를 적용한 **CorefBERT-Colloquial** 모델이 본 연구의 baseline으로 사용되었다.
- **CorefBERT:** Fang 등(2020)이 제안한 **언어모델 변형**으로, **coreference resolution** 신호를 사전학습에 통합하여, 대명사 지시 대상 예측 등을 추가 학습시킨 BERT 모델이다. 본 논문에서는 이 CorefBERT-base를 가져와 colloquial claims 데이터로 fine-tune하여, coreference를 잘 처리하는 사실검증 모델로 활용했다.
- **Dense Passage Retrieval (DPR):** Karpukhin 등【(2020)】의 **밀도기반 문서 검색** 기법. 쿼리와 문서를 각각 BERT 등으로 임베딩하고, 내적(dot product) 유사도로 대규모 문서 코퍼스에서 가장 유사한 문서를 찾는다. 보통 open-domain QA와 fact-checking에서 사용되며, 여기서는 위키백과 전체를 인덱싱한 후 대화 응답+문맥을 쿼리로 top-100 문서를 찾는 데 쓰였다.
- **BM25:** 정보검색에서 널리 쓰이는 **빈도 기반** 문서 랭킹 알고리즘. 질의와 문서 간 단어 일치 횟수와 문서 길이 등을 고려해 점수를 산정한다. 본 연구에서는 MediaWiki API로 검색된 문서들 내 문장들을 BM25로 재정렬하여 증거 후보를 선택하는데 사용하였다.
- **Sense2Vec:** Trask 등(2015)의 **단어 의미 임베딩** 모델. 단어의 품사나 의미별로 임베딩 공간을 만들어, 유사한 “sense”를 가진 단어를 벡터 유사도로 찾을 수 있다. 여기서는 예를 들어 “Apple”이라는 단어의 인명 의미 vs 회사 의미를 구분해, 문맥에 맞는 유사 엔티티를 찾는 데 활용했다.
- **T5:** Text-to-Text Transfer Transformer 모델【(Raffel et al., 2020)】. 대규모 텍스트 변환에 특화된 구글의 언어모델. 본 연구에서는 T5-base를 **masked language model**처럼 활용하여, evidence+문맥을 조건으로 응답 문장의 빈칸을 채우는 방식으로 claim을 생성했다.
- **BlenderBot:** Facebook AI가 공개한 대규모 **대화 생성모델**【(Roller et al., 2021)】. 인간과 유사한 대화를 생성하도록 9.4B 규모로 훈련된 버전 등이 있으며, 본 연구에서는 그 smaller version을 Wizard-of-Wikipedia 데이터에 맞춤 파인튜닝해 사용했다. 이를 통해 새로운 증거 상황에 대응하는 문장 생성에 활용함.
- **Krippendorff’s alpha:** 라벨링 작업자들 간 **일치도(inter-annotator agreement)**를 측정하는 지표. 1에 가까울수록 완전 합의, 0에 가까울수록 랜덤 수준임을 뜻한다. 본 논문에서 카테고리 분류 작업에 α≈0.6 정도가 나왔는데, 이는 **중간 정도 신뢰도**로 해석된다.
- **Local Mutual Information (LMI):** 특정 단어나 구문이 레이블과 얼마나 밀접히 연관되는지 나타내는 통계량【(Schuster et al., 2019)】. LMI가 높으면 해당 단어가 그 레이블에 **편향적 힌트**로 작용함을 의미한다. 예컨대 FEVER에선 “not”의 LMI(Refuted)가 높았는데, DialFact에선 그런 경향이 낮게 제어되었다.
- **Aug-WoW:** 본 논문의 **주요 제안 모델**로, “Augmented WoW”의 약자. Wizard-of-Wikipedia 기반으로 **증강 학습 데이터**를 만들고 **BERT-base**로 학습한 **claim verification** 모델이다. 3개 하위태스크 중 최종 단계에 해당하며, 대화 응답의 사실성(3분류)을 예측한다. 데이터 효율적인 약지도 방식으로 기존 대비 높은 성능을 보였다.

## 관련자료 링크
- **GitHub 저장소:** [salesforce/DialFact](https://github.com/salesforce/DialFact) – DialFact 데이터셋 및 증거 검색/모델 코드 공개 레포지토리
- **논문 (ACL 2022):** [ACL Anthology](https://aclanthology.org/2022.acl-long.263) / [arXiv](https://arxiv.org/abs/2110.08222) – 논문 PDF와 세부 내용
- **Papers with Code:** DialFact [데이터셋 페이지](https://paperswithcode.com/dataset/dialfact) – DialFact 개요 및 코드 구현 목록
- **관련 데이터셋:** Wizard-of-Wikipedia [ParlAI](https://parl.ai/projects/wizard_of_wikipedia/) – DialFact 구축에 사용된 대화 데이터셋 소개
- **후속 연구:** *Automated Fact-Checking in Dialogue: Are Specialized Models Needed?* (Huang et al., 2023) – DialFact를 활용하여 대화용 특화 모델과 범용 모델 성능을 비교한 연구

## 논문의 기여 및 한계
- **학술적 기여:** 본 연구는 **대화 시스템의 사실성 검증**이라는 새로운 문제 설정을 제시하고, 이를 평가할 수 있는 **대규모 공개 데이터셋 DialFact**를 구축하였다. 이를 통해 그간 부족했던 **대화 도메인 사실검증 연구의 기반**을 마련한 공로가 있다. 또한 검증 가능 여부 판정, 증거 검색, 사실 판정의 **체계적인 평가 프레임워크**를 확립하여, 후속 연구들이 부분별 성능 개선 및 오류 원인 분석을 수행할 수 있는 표준 환경을 제공했다. 특히 **기존 SOTA 모델들이 대화에 취약**함을 실증하고, **약한 지도 데이터 생성**을 통해 이를 향상시킬 수 있음을 보인 것은 이론적으로도 의미가 크다. 요약하면, DialFact는 **사실 검증 연구를 문서 단위에서 대화 단위로 확장**하고, 향후 **대화 에이전트의 신뢰성 평가**에 필수적인 벤치마크로 자리잡을 수 있다.
- **실용적 기여:** DialFact에서 제안한 기법들은 실제 **챗봇 시스템 개선**에 바로 응용 가능하다. 예를 들어, 챗봇이 응답 생성 후 **Verifiable 여부를 판별**하고, 필요시 **증거를 찾아 사실 여부를 검토**한 뒤 사용자에게 **팩트 체크 결과**를 알려주는 파이프라인에 이번 연구 결과를 활용할 수 있다. 또한 본 연구의 **증거 생성/검색 기법**(wiki API 활용, sense2vec로 개체 확장 등)은 지식 기반 대화 시스템 전반에 적용되어 **정보 검색 성능**을 높일 수 있다. 궁극적으로, DialFact는 **안전하고 신뢰할 수 있는 대화형 AI** 개발을 위한 중요한 데이터 자산이며, 추후 기업형 가상비서나 정보 제공 챗봇에 **사실성 필터**를 도입하는데 기여할 수 있다.
- **한계:** 첫째, DialFact 데이터셋은 **위키피디아 지식 범위**에 한정되어 있어, 일상 대화 중 **세계 지식 밖**의 내용(예: 개인 경험담 등)은 다루지 않는다. 따라서 현실의 모든 대화에 일반화되진 않을 수 있다. 둘째, **대화 맥락 처리**의 한계가 남아 있다. 현 모델은 마지막 발화만을 검증하지만, 실제로는 앞의 대화 흐름에서 축적된 정보가 필요할 수 있다. (DialFact에서도 문맥 일부는 사용되나, 긴 문맥이나 멀티턴 추론은 미포함됨.) 셋째, **레이블 모호성** 문제: 앞서 언급한 바와 같이 의견 vs 사실 경계에 놓인 문장 등 **Annotator 간 불일치** 사례가 있었고, 이는 모델 학습에도 어려움을 준다. 이런 애매 케이스를 별도로 처리할 방법은 제시되지 않았다. 넷째, 제안한 Aug-WoW 모델은 특정 생성 데이터에 특화되어 **상대적으로 단순한 BERT 분류기**라서, 구조적으로 혁신적이진 않다. 향후 더 복잡한 추론(예: multi-hop)이나 **생성적 판별**이 필요한 경우엔 본 모델로 한계가 있을 수 있다. 다섯째, pipeline 구조 특성상 **전단계 오류 전파** 문제가 있다: 예를 들어 evidence retrieval이 실패하면 이후 verification은 무의미해지는데, end-to-end로 공동 최적화하지는 못했다. 이로 인해 실제 응용에서 어느 한 단계 성능이 낮으면 전체 성능이 좌우되는 한계가 있다. 마지막으로, 실험에서 **70% 미만의 정확도**밖에 달성 못한 부분(특히 Refuted/NEI 구분)은 아직 해결되지 않은 채 남아있다. 이는 대화 사실검증이 현존 모델에 매우 어려운 과제임을 다시금 보여주는 것으로, **절대 성능 개선**이 향후 과제로 남는다.
- **향후 과제:** 위 한계들을 극복하기 위해 몇 가지 연구 방향이 제시된다. (1) **대화 맥락의 활용 확대:** 여러 턴에 흩어진 정보를 모아 추론하거나, coreference를 명시적으로 해소하는 모델이 필요하다. 예를 들어, **대화 요약 + 사실검증** 결합이나, **Graph neural network**로 문맥 간 관계를 파악하는 접근을 시도할 수 있다. (2) **Joint 모델링:** verifiable 판정부터 evidence retrieval, verification까지 **엔드투엔드 훈련**되는 모델을 개발하면 단계별 오류를 줄일 수 있을 것이다. 최근 retrieval-augmented generation 모델이나, multi-task learning 기법을 적용해볼 수 있다. (3) **대규모 사전학습 모델 활용:** GPT-4와 같은 최신 거대 언어모델은 자체적으로 상식과 사실 지식을 어느 정도 갖추고 있어, 특별한 학습 없이도 DialFact 같은 과제를 수행할 가능성이 있다. 향후 이러한 **LLM**들을 DialFact에서 평가하고, 필요시 **피드백 거절 학습** 등을 통해 fact-checking 능력을 강화하는 연구가 기대된다. (4) **다중 도메인 및 다국어 확장:** 현 DialFact는 영어 위키 지식에 한정되지만, 동일 컨셉을 다른 언어 대화나, 예를 들면 의료 상담, 법률 자문 등의 **특화 도메인** 대화로 확장할 수 있다. 각 도메인별 fact-checking 벤치마크를 구축하면 분야별 챗봇 신뢰성을 높이는 데 도움이 될 것이다. (5) **실시간 상호작용**: 최종적으로 사용자와 챗봇 간 대화 도중에 사실 검증을 수행하여, 잘못된 정보가 탐지되면 챗봇이 스스로 정정하거나 출처를 제시하는 **interactive fact-checking** 기능 구현이 목표가 될 수 있다. DialFact는 그 첫걸음으로서, 이런 응용 연구들을 가능하게 해주는 발판이 될 전망이다.

## 추가
논문에 직접 언급되진 않았지만 주목할 만한 점으로, **대화형 사실 검증과 대형 언어모델의 만남**을 들 수 있다. DialFact가 등장한 이후, 파운데이션 모델의 급격한 발전으로 ChatGPT 같은 모델들도 등장했는데, 이러한 **초거대 모델**들은 방대한 지식을 내재하고 있어 별도 검색 없이도 어느 정도 사실 검증적 응답을 생성할 수 있다. 그러나 동시에 환각 정보도 빈번히 생성하므로, DialFact와 같은 **평가 세트**로 최신 모델들의 성능을 재점검하는 일이 중요해졌다. 실제로 Huang 등(2023)은 DialFact를 포함한 여러 벤치마크에서 GPT-3 등의 성능을 측정하며, **대화 특화 모델 vs 범용 모델**의 우열을 분석했다. 향후에는 **Retrieval-Augmented Generation** 기법으로 거대 모델에 실시간 검색 능력을 부여하고, DialFact 태스크를 end-to-end로 해결하도록 하는 연구도 기대된다. 또한 DialFact는 **대화 에이전트의 신뢰도 평가 지표**로 활용될 수 있다. 예컨대 챗봇의 대화 로그에 DialFact와 유사한 검증이 자동으로 수행돼 **팩트 스코어**를 산출함으로써, 사용자에게 해당 봇의 정보 신뢰도를 제공하는 방식이다. 마지막으로, **다른 사실성 평가 benchmark와의 연계**도 고려해볼만 하다. 현재 요약문 사실성 검증(QAGS 등)이나 생성 문장 논리일관성 평가 등이 활발한데, DialFact와 교차로 모델을 학습하거나 공유 태스크로 다룬다면 **멀티도메인 사실검증** 능력을 향상시킬 가능성이 있다. 요컨대 DialFact는 하나의 완결된 데이터셋일 뿐만 아니라, **대화 AI의 사실성** 문제 전반에 걸쳐 후속 연구를 촉발하는 촉매제 역할을 하고 있으며, 실제 산업계 챗봇의 사실 오류를 줄이는 실용적 노력에도 큰 영향을 미칠 것으로 보인다.
