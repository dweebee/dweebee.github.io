---
title: "Attention"
last_modified_at: 2023-01-03

tags:
  - attention
  - transformer
toc: true
toc_sticky: true

categories: anytopic
published: true
---

# 트랜스포머 어텐션 완전 정복: Scaled Dot-Product Attention (계산 예시 포함)

## 1. 어텐션이란?

어텐션은 문장에서 어떤 단어가 현재 단어와 **얼마나 관련이 깊은지(유사도)**를 계산한 뒤,  
그 **중요도에 따라 정보(Value)를 가중합**해서 문맥 정보를 반영하는 기법이다.

---

## 2. 예시 문장

문장:  
**The cat sat on the mat**

총 6개의 단어가 있으며, 각각 토큰으로 나누어 처리된다.

우리는 현재 단어 **"sat"**에 대해 어텐션을 계산할 것이다.  
즉, "sat"이 문장에서 다른 단어들로부터 어떤 정보를 얼마큼 받아야 하는지를 계산한다.

---

## 3. Q, K, V 정의 및 예시

각 단어는 모델에 의해 일정한 차원의 벡터로 변환된다 (예: d_model = 4).  
각 단어 벡터로부터 다음 3가지가 생성된다:

- Q: 현재 단어의 query (정보를 찾는 기준)
- K: 각 단어의 key (정보를 제공할 수 있는 열쇠)
- V: 각 단어의 value (제공할 정보)

### 예시 (차원: d_model = 4)

| Token | Q (1x4)            | K (1x4)            | V (1x4)            |
|-------|--------------------|--------------------|--------------------|
| the₁  | [1, 0, 1, 0]       | [1, 0, 1, 0]       | [0.1, 0.0, 0.1, 0.0]|
| cat   | [0, 1, 0, 1]       | [0, 1, 0, 1]       | [0.0, 0.2, 0.0, 0.2]|
| sat   | [1, 1, 1, 1]       | [1, 1, 1, 1]       | [0.3, 0.3, 0.3, 0.3]|
| on    | [0, 0, 1, 1]       | [0, 0, 1, 1]       | [0.0, 0.0, 0.1, 0.1]|
| the₂  | [1, 0, 0, 1]       | [1, 0, 0, 1]       | [0.1, 0.0, 0.0, 0.1]|
| mat   | [1, 1, 0, 0]       | [1, 1, 0, 0]       | [0.2, 0.2, 0.0, 0.0]|

---

## 4. 어텐션 계산 (Query = "sat")

### 1단계: 유사도 계산 (Q · Kᵀ)

현재 Query = [1, 1, 1, 1]  
각 단어의 Key와 내적(dot product) 수행:

```
Q · Kᵢ = sum(Q[i] * Kᵢ[i])
```

| Token | Q·Kᵢ = 유사도 점수 |
|-------|------------------|
| the₁  | 1+0+1+0 = 2      |
| cat   | 0+1+0+1 = 2      |
| sat   | 1+1+1+1 = 4      |
| on    | 0+0+1+1 = 2      |
| the₂  | 1+0+0+1 = 2      |
| mat   | 1+1+0+0 = 2      |

→ 결과 shape: (1 x 6)

---

### 2단계: 스케일 조정

모델 차원(dk) = 4 → √dk = 2

→ 점수를 2로 나눈다:

| Token | scaled score = Q·Kᵢ / 2 |
|-------|------------------------|
| all   | 2/2 = 1.0 (sat만 4/2=2.0) |

→ scaled scores = [1.0, 1.0, 2.0, 1.0, 1.0, 1.0]

---

### 3단계: softmax 적용

softmax(xᵢ) = exp(xᵢ) / sum(exp(xⱼ))

```
exp(1.0) ≈ 2.718, exp(2.0) ≈ 7.389

softmax = [2.718, 2.718, 7.389, 2.718, 2.718, 2.718]
sum ≈ 20.979

attention_weights = 각 값 / 20.979
```

| Token | softmax 비율 (approx.) |
|-------|------------------------|
| all (except sat) | ≈ 0.1296     |
| sat             | ≈ 0.352      |

→ attention_weights shape: (1 x 6)

---

### 4단계: 가중합 계산 (attention weights × V)

각 단어의 Value 벡터에 softmax 가중치를 곱하고 더한다:

예시:

```
context_vector = 
0.13*[0.1, 0.0, 0.1, 0.0] +
0.13*[0.0, 0.2, 0.0, 0.2] +
0.35*[0.3, 0.3, 0.3, 0.3] +
0.13*[0.0, 0.0, 0.1, 0.1] +
0.13*[0.1, 0.0, 0.0, 0.1] +
0.13*[0.2, 0.2, 0.0, 0.0]
```

→ 결과는 shape (1 x 4)의 context vector

---

## 5. 전체 과정 요약 (shape 포함)

| 단계 | 연산 | shape |
|------|------|--------|
| Q · Kᵀ | (1 x 4) × (6 x 4)ᵀ → (1 x 6) | 유사도 점수 |
| 나누기 √dk | (1 x 6) | 스케일 조정 |
| softmax 적용 | (1 x 6) | 중요도 확률 |
| softmax × V | (1 x 6) × (6 x 4) → (1 x 4) | context vector 생성 |

---

## 6. 핵심 정리

- **Q**는 "지금 이 단어가 어떤 정보를 원하나?"를 나타냄
- **K**는 "내가 가진 정보의 특징은 이것이다"
- **Q·Kᵀ**는 "너와 나의 관련도"
- **softmax**는 중요도를 정규화
- **V**는 실제 정보를 담고, softmax로 가중합
- **출력은 context vector**이며, 현재 단어의 문맥 정보를 반영한 표현


