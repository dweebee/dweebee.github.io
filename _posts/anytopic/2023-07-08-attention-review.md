---
title: "Attention Review"
last_modified_at: 2023-07-08

tags:
  - attention
toc: true
toc_sticky: true

categories: anytopic
published: true
---

### 순환 신경망을 사용한 인코더-디코더 구조

순환 신경망은 텍스트같은 시퀀스 데이터를 효과적으로 처리할 수 있지만 여러 한계를 갖고 있다. 대표적으로 시퀀스가 길어질수록 이전에 처리한 데이터를 기억하기 어렵다는 점이다. 이 문제를 해결하기 위해 LSTM, GRU 등이 개발되었지만 완벽한 해결책은 아니었다.

이런 한계점은 '기계 번역'에서 두드러졌는데, 번역할 문장이 길어질수록 RNN기반 모델은 번역 품질이 떨어졌다.
기계 번역에 사용되는 신경망 구조는 전형적으로 시퀀스-투-시퀀스인데, 이 구조는 텍스트를 받아 변환된 텍스트를 출력하는 구조이다. 대표적으로 번역과 요약이 있다. 이런 작업은 인코더-디코더 구조를 사용하며, 인코더와 디코더에 각각 순환 신경망을 적용한다. 

##### 예
나는 너를 사랑해란 문장을 영어로 번역한다면, 세 토큰이 각 셀에 들어가는 '인코더 순환신경망'이 있다.
마지막 은닉상태가 '디코더 순환신경망'에 전달된다.
I love you 세 토큰이 각 셀로부터 번역되어 출력된다.
일반적으로 인코더와 디코더의 문장 길이(토큰 수)는 다를 수 있다.

<정리> 인코더 신경망은 입력된 문장을 토큰 단위로 하나씩 처리하면서 전체 정보를 하나의 은닉 상태에 압축한다.
그 다음, 디코더 신경망이 이 은닉 상태를 받아 마찬가지로 한 단어씩 번역된 문장을 생성한다.

위와 같은 구조는 번역할 문장이 길어질수록 초기에 입력된 내용을 기억하기 어려워진다.
특히, 디코더 순환신경망은 인코더의 마지막 은닉 상태만 참고하여 번역을 수행하기 때문에 이런 문제가 더 심각해진다.
또, 인코더와 디코더는 각 토큰마다 처리해서 매우 느리다.

<디코더의 생성 과정>
(1) I라는 토큰을 생성 - 인코더의 마지막 은닉상태와 자신의 은닉상태를 활용해 love 토큰을 생성
(2) I love라는 토큰 생성 - 인코더의 마지막 은닉상태와 자신의 은닉상태를 활용해 you 토큰 생성
(3) .... "디코더는 자신이 이전에 생성한 토큰들과 인코더의 마지막 은닉상태" 이 두가지를 활용해 다음 토큰 생성

디코더처럼 이전에 생성한 토큰을 참고하면서 다음 토큰을 생성하는 것을 "자기회귀 모델"이라 한다.

### 어텐션 메커니즘(2014)

기존에는 디코더가 {자신이 생성한 이전 토큰들} + {인코더의 마지막 은닉상태} 이 두가지를 활용해 번역을 수행했지만,
어텐션을 사용하면 인코더의 모든 타임스텝에서 계산된 은닉상태를 활용할 수 있다.

핵심은 "인코더의 모든 타임스템에서 계산된 은닉상태".

(WIP)